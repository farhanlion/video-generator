# ğŸµ Emotion-Aware Music Video Generator

This is a Flask-based web application that automatically generates cinematic music videos from audio input using AI-driven transcription, emotion detection, and video generation.

---

## ğŸŒŸ Features

- ğŸ§ Upload a song (MP3/WAV)
- ğŸ“ Transcribe lyrics using OpenAI Whisper
- ğŸµ Extract the chorus using `pychorus` (built on Librosa)
- ğŸ­ Detect emotions from both audio and lyrics
- âœ¨ Generate detailed video prompts using GPT-4o or Gemini 1.5 Flash
- ğŸ¥ Create cinematic scenes using Google's Veo (8s Ã— 2)
- ğŸ¬ Stitch clips + audio using MoviePy
- ğŸ”— Watch or download the final video directly from your browser

---

## ğŸ§  AI & Tools Used

| Tool / API              | Purpose                                         |
|-------------------------|-------------------------------------------------|
| OpenAI Whisper API      | Transcribe lyrics from audio                    |
| pychorus + Librosa      | Detect and extract chorus                       |
| Hugging Face (custom)   | Detect emotion from lyrics                      |
| music2emotion API       | Detect emotion from audio                       |
| GPT-4o / Gemini 1.5 API | Generate cinematic video prompts                |
| Google Veo 2.0          | Generate video scenes from prompts              |
| MoviePy                 | Stitch video clips and overlay audio            |
| Flask                   | Web backend                                     |
| HTML5 + CSS + JavaScript| Frontend interface                              |

---

## ğŸ“¦ Folder Structure

```
â”œâ”€â”€ app.py # Flask backend entry point
â”œâ”€â”€ templates/ # HTML templates
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ uploads/ # Uploaded and processed files
â”‚   â”œâ”€â”€ videos/ # Generated video outputs
â”‚   â””â”€â”€ style/ # CSS and static assets
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ transcribe.py # Whisper transcription logic
â”‚   â”œâ”€â”€ emotion_audio.py # Audio emotion detection
â”‚   â”œâ”€â”€ emotion_lyrics.py # Lyrics emotion classification
â”‚   â”œâ”€â”€ postprocess.py # Stitching videos and audio
â”‚   â”œâ”€â”€ chorus.py # Chorus extraction using pychorus
â”‚   â”œâ”€â”€ prompt_gen.py # Prompt creation using GPT or Gemini
â”‚   â””â”€â”€ video_gen.py # Google Veo video generation
â”œâ”€â”€ requirements.txt # Python package dependencies
â”œâ”€â”€ .env # API keys and secrets
â””â”€â”€ README.md # This file
```

---

## ğŸš€ Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/emotion-music-generator.git
cd emotion-music-generator
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Add Environment Variables

Create a `.env` file in the root directory and add:

```env
OPENAI_API_KEY=your_open_ai_key
GOOGLE_CLOUD_PROJECT=your_gcp_project
GOOGLE_STORAGE_BUCKET=gs://your-storage-bucket
```

### 4. Run the App

```bash
python app.py
```

Visit [http://localhost:5000](http://localhost:5000) in your browser.

---

## ğŸ§ª Sample Use Case

1. User uploads a song (e.g., `enchanted.mp3`)
2. App transcribes lyrics and extracts chorus
3. Emotion is detected from both audio and lyrics
4. Prompts are generated and fed to Google Veo to create two 8s clips
5. Final video is stitched with chorus audio and displayed to the user

---

## ğŸ“Œ Notes

- Output video is fixed to 16 seconds (two 8s clips).
- The generated character in the video is a stylized human-like capybara ğŸ¾.
- This is an academic/experimental prototype and depends on API access to OpenAI, Google Veo, and Hugging Face models.
